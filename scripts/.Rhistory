breaks = legend_breaks,
labels = legend_labels) +
labs(x = "X Value", y = "Probability P(X=x)",
title = "Empirical Distribution vs. True PDF")
alpha_mle = 1 + nrow(raw_data)/sum(log(raw_data$x/x_min))
legend_breaks = c("Empirical", "LS_MLE", "PDF")
legend_labels = c("Empirical",expression(paste(alpha[ls_mle])), "True PDF")
agg_data %>%
mutate(LS_MLE = ((alpha_mle - 1)/x_min)*(x/x_min)^(-alpha_mle)) %>%
gather(distribution_type, prob, Empirical:LS_MLE) %>%
ggplot(mapping = aes(x = x, y = prob, color = distribution_type)) +
geom_point() +
scale_x_log10(labels = scales::comma) +
scale_y_log10(limits = c(0.000001,1)) +
scale_color_discrete(name = "Distribution",
breaks = legend_breaks,
labels = legend_labels) +
labs(x = "X Value", y = "Probability P(X=x)",
title = "Empirical Distribution vs. True PDF")
# Define Constants
n_samples <- 100000
alpha <- 2
x_min <- 1
# Generate data
set.seed(2)
raw_data <-
tibble(u = runif(n_samples),
x = round(x_min * u^(1/(1-alpha))))
agg_data <-
raw_data %>%
count(x) %>%
mutate(Empirical = n/sum(n),
PDF = ((alpha - 1)/x_min)*(x/x_min)^(-alpha))
legend_breaks = c("Empirical", "PDF")
legend_labels = c("Empirical","True PDF")
agg_data %>%
gather(distribution_type, prob, Empirical:PDF) %>%
ggplot(mapping = aes(x = x, y = prob, color = distribution_type)) +
geom_point() +
scale_x_log10(labels = scales::comma) +
scale_y_log10(limits = c(0.000001,1)) +
scale_color_discrete(name = "Distribution",
labels = legend_labels,
breaks = legend_breaks) +
labs(x = "X Value", y = "Probability P(X=x)",
title = "Empirical Distribution vs. True PDF")
# Estimate alpha using OLS
alpha_ls <- -lm(log10(Empirical) ~ log10(x), data = agg_data)$coefficients[["log10(x)"]]
alpha_ls
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
legend_breaks = c("Empirical", "PDF")
legend_labels = c("Empirical","True PDF")
agg_data %>%
gather(distribution_type, prob, Empirical:PDF) %>%
ggplot(mapping = aes(x = x, y = prob, color = distribution_type)) +
geom_point() +
scale_x_log10(labels = scales::comma) +
scale_y_log10(limits = c(0.000001,1)) +
scale_color_discrete(name = "Distribution",
labels = legend_labels,
breaks = legend_breaks) +
labs(x = "X Value", y = "Probability P(X=x)",
title = "Empirical Distribution vs. True PDF")
legend_breaks = c("Empirical", "PDF")
legend_labels = c("Empirical","True PDF")
agg_data %>%
gather(distribution_type, prob, Empirical:PDF) %>%
ggplot(mapping = aes(x = x, y = prob, color = distribution_type)) +
geom_point() +
scale_x_log10(labels = scales::comma) +
scale_y_log10(limits = c(0.000001,1)) +
scale_color_discrete(name = "Distribution",
labels = legend_labels,
breaks = legend_breaks) +
labs(x = "X Value", y = "Probability P(X=x)",
title = "Empirical Distribution vs. True PDF")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Initialize libraries
library(stringr)
library(lubridate)
library(knitr)
library(caret)
library(tidyverse)
# Define input file
file_in <- "../data/train.csv"
# Read in data
df <- read_csv(file_in) %>%
dmap_at("date", ~ymd(.x))
df <- df %>%
# Recode "waterfront" to factor
dmap_at("waterfront", as.factor) %>%
mutate(
years_since_renovation = ifelse(yr_renovated == 0, 0, yr_renovated - yr_built),
sale_year = year(date),
sale_month = month(date),
yard_size = sqft_lot - sqft_living,
house_age = sale_year - yr_built,
sale_season = ifelse(sale_month <= 4, "Winter",
ifelse(sale_month <= 5, "Spring",
ifelse(sale_month <= 8, "Summer",
ifelse(sale_month <= 12, "Fall")))),
size_vs_neighbors = sqft_living/sqft_living15,
price_per_sqft = price/sqft_living
) %>%
# Remove extraneoys variables
select(-c(id, date, yr_built, yr_renovated, zipcode, lat, long, sale_year, sale_month,
price_per_sqft, size_vs_neighbors))
# Split data into train and validation sets
percent_in_train <- 0.7
train_indicies <- sample(nrow(df), size = percent_in_train*nrow(df))
train <- df[train_indicies, ]
validation <- df[-train_indicies, ]
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale", "nzv")
# Apply same pre-processing steps to validation set
preProcessObject <- preProcess(train, method = preProcessSteps)
validation <- predict(preProcessObject, validation)
set.seed(1234)
rfe.cntrl <- rfeControl(functions = rfFuncs,
method = "cv",
number = 10)
train.cntrl <- trainControl(selectionFunction = "oneSE")
# Commented out to speed up runtime
# rfe.results <- rfe(price~., train,
#                rfeControl = rfe.cntrl,
#                preProc = preProcessSteps,
#                metric = "RMSE",
#               trControl = train.cntrl)
rfe.results <- read_rds("../models/rfe.results.rds")
print(rfe.results)
cvCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
selectionFunction = "oneSE")
lm.fit <- lm(price~., data = train)
# Define tuning paramter grid
rfGrid <- expand.grid(.mtry = c(4,5,6,7))
# Fit random forest model
set.seed(1234)
# rf.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "rf",
#                 tuneGrid = rfGrid,
#                 trControl = cvCtrl)
rf.fit <- read_rds("../models/rf.fit.rds")
predict.train(rf.fit, newdata = validation)
preProcessSteps <- c("center", "scale")
preProcessObject <- preProcess(train, method = preProcessSteps)
validation <- predict(preProcessObject, validation)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Initialize libraries
library(stringr)
library(lubridate)
library(knitr)
library(caret)
library(tidyverse)
# Define input file
file_in <- "../data/train.csv"
# Read in data
df <- read_csv(file_in) %>%
dmap_at("date", ~ymd(.x))
df <- df %>%
# Recode "waterfront" to factor
dmap_at("waterfront", as.factor) %>%
mutate(
years_since_renovation = ifelse(yr_renovated == 0, 0, yr_renovated - yr_built),
sale_year = year(date),
sale_month = month(date),
yard_size = sqft_lot - sqft_living,
house_age = sale_year - yr_built,
sale_season = ifelse(sale_month <= 4, "Winter",
ifelse(sale_month <= 5, "Spring",
ifelse(sale_month <= 8, "Summer",
ifelse(sale_month <= 12, "Fall")))),
size_vs_neighbors = sqft_living/sqft_living15,
price_per_sqft = price/sqft_living
) %>%
# Remove extraneoys variables
select(-c(id, date, yr_built, yr_renovated, zipcode, lat, long, sale_year, sale_month,
price_per_sqft, size_vs_neighbors))
# Split data into train and validation sets
percent_in_train <- 0.7
train_indicies <- sample(nrow(df), size = percent_in_train*nrow(df))
train <- df[train_indicies, ]
validation <- df[-train_indicies, ]
preProcessSteps <- c("center", "scale")
preProcessObject <- preProcess(train, method = preProcessSteps)
validation <- predict(preProcessObject, validation)
names(validation)
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale")
# Apply same pre-processing steps to validation set
preProcessObject <- preProcess(train, method = preProcessSteps)
validation <- predict(preProcessObject, validation)
# Define tuning paramter grid
rfGrid <- expand.grid(.mtry = c(4,5,6,7))
# Fit random forest model
set.seed(1234)
# rf.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "rf",
#                 tuneGrid = rfGrid,
#                 trControl = cvCtrl)
rf.fit <- read_rds("../models/rf.fit.rds")
predict.train(rf.fit, newdata = validation)
# Define grid of tuning parameters
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
n.trees = (1:20)*100,
shrinkage = seq(.0005, .05, .005),
n.minobsinnode = 10)
# Fit GBM model
set.seed(825)
# gbm.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "gbm",
#                 tuneGrid = gbmGrid,
#                 trControl = cvCtrl)
gbm.fit <- read_rds("../models/gbm.fit.rds")
predict.train(gbm.fit, validation)
# Define grid of tuning parameters
tuneGrid <- expand.grid(.alpha = seq(0, 1, 0.1),
.lambda = seq(0, 0.05, by = 0.005))
# Fit penalized logistic regression model (elastic net)
set.seed(1234)
elastic.fit <- train(price ~ .,
data = train,
preProc = preProcessSteps,
method = "glmnet",
# tuneGrid = tuneGrid,
trControl = cvCtrl)
elastic.fit
evalResults <- tibble(LM = predict.train(lm.fit, validation),
ELASTIC = predict.train(elastic.fit, validation),
RF = predict.train(rf.fit, validation),
GBM = predict.train(gbm.fit, validation))
evalResults <- tibble(LM = predict.train(lm.fit, validation),
ELASTIC = predict.train(elastic.fit, validation),
RF = predict.train(rf.fit, validation),
GBM = predict.train(gbm.fit, validation))
evalResults <- tibble(LM = predict.train(lm.fit, newdata = validation),
ELASTIC = predict.train(elastic.fit, newdata = validation),
RF = predict.train(rf.fit, newdata = validation),
GBM = predict.train(gbm.fit, newdata = validation))
predict.train(lm.fit, newdata = validation)
predict.train(elastic.fit, newdata = validation)
evalResults <- tibble(LM = predict(lm.fit, newdata = validation),
ELASTIC = predict.train(elastic.fit, newdata = validation),
RF = predict.train(rf.fit, newdata = validation),
GBM = predict.train(gbm.fit, newdata = validation))
head(evalResults)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Initialize libraries
library(stringr)
library(lubridate)
library(knitr)
library(caret)
library(tidyverse)
# Define input file
file_in <- "../data/train.csv"
# Read in data
df <- read_csv(file_in) %>%
dmap_at("date", ~ymd(.x))
df <- df %>%
# Recode "waterfront" to factor
dmap_at("waterfront", as.factor) %>%
mutate(
years_since_renovation = ifelse(yr_renovated == 0, 0, yr_renovated - yr_built),
sale_year = year(date),
sale_month = month(date),
yard_size = sqft_lot - sqft_living,
house_age = sale_year - yr_built,
sale_season = ifelse(sale_month <= 4, "Winter",
ifelse(sale_month <= 5, "Spring",
ifelse(sale_month <= 8, "Summer",
ifelse(sale_month <= 12, "Fall")))),
size_vs_neighbors = sqft_living/sqft_living15,
price_per_sqft = price/sqft_living
) %>%
# Remove extraneoys variables
select(-c(id, date, yr_built, yr_renovated, zipcode, lat, long, sale_year, sale_month,
price_per_sqft, size_vs_neighbors))
# Split data into train and validation sets
percent_in_train <- 0.7
train_indicies <- sample(nrow(df), size = percent_in_train*nrow(df))
train <- df[train_indicies, ]
validation <- df[-train_indicies, ]
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale")
# Apply same pre-processing steps to validation set
preProcessObject <- preProcess(train, method = preProcessSteps)
validation <- predict(preProcessObject, validation)
set.seed(1234)
rfe.cntrl <- rfeControl(functions = rfFuncs,
method = "cv",
number = 10)
train.cntrl <- trainControl(selectionFunction = "oneSE")
# Commented out to speed up runtime
# rfe.results <- rfe(price~., train,
#                rfeControl = rfe.cntrl,
#                preProc = preProcessSteps,
#                metric = "RMSE",
#               trControl = train.cntrl)
rfe.results <- read_rds("../models/rfe.results.rds")
print(rfe.results)
ggplot(rfe.results) +
labs(title = "Recursive Feature Elimination\nNumber of Variables vs. RMSE")
data_frame(predictor = rownames(varImp(rfe.results)),
var_imp = varImp(rfe.results)$Overall) %>%
ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "", y = "Variable Importance",
title = "Recursive Feature Elimination Variable Importance")
cvCtrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
selectionFunction = "oneSE")
lm.fit <- lm(price~., data = train)
# Define grid of tuning parameters
tuneGrid <- expand.grid(.alpha = seq(0, 1, 0.1),
.lambda = seq(0, 0.05, by = 0.005))
# Fit penalized logistic regression model (elastic net)
set.seed(1234)
elastic.fit <- train(price ~ .,
data = train,
preProc = preProcessSteps,
method = "glmnet",
# tuneGrid = tuneGrid,
trControl = cvCtrl)
# Define tuning paramter grid
rfGrid <- expand.grid(.mtry = c(4,5,6,7))
# Fit random forest model
set.seed(1234)
# rf.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "rf",
#                 tuneGrid = rfGrid,
#                 trControl = cvCtrl)
rf.fit <- read_rds("../models/rf.fit.rds")
# Define grid of tuning parameters
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
n.trees = (1:20)*100,
shrinkage = seq(.0005, .05, .005),
n.minobsinnode = 10)
# Fit GBM model
set.seed(825)
# gbm.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "gbm",
#                 tuneGrid = gbmGrid,
#                 trControl = cvCtrl)
gbm.fit <- read_rds("../models/gbm.fit.rds")
evalResults <- tibble(LM = predict(lm.fit, newdata = validation),
ELASTIC = predict.train(elastic.fit, newdata = validation),
RF = predict.train(rf.fit, newdata = validation),
GBM = predict.train(gbm.fit, newdata = validation))
head(evalResults)
setwd("~/Desktop/fall_2016/MS&E_226/ms&e226_collaboration/scripts")
write_rds(elastic.fit, "../models/elastic.fit.rds")
elastic.fit <- read_rds("../models/elastic.fit.rds")
evalResults <- tibble(#LM = predict(lm.fit, newdata = validation),
#ELASTIC = predict.train(elastic.fit, newdata = validation),
RF = predict.train(rf.fit, newdata = validation),
GBM = predict.train(gbm.fit, newdata = validation))
evalResults %>%
gather(model_type, prediction, RF:GBM) %>%
group_by(model_type) %>%
summarize(rmse = RMSE(pred = prediction, obs = test$price)) %>%
ggplot(mapping = aes(x = model_type, y = rmse)) +
geom_bar(stat = "identity")
evalResults %>%
gather(model_type, prediction, RF:GBM)
library(caret)
install.packages("hydroGOF")
library(hydroGOF)
library(lubridate)
library(knitr)
library(knitr)
library(caret)
library(tidyverse)
evalResults %>%
gather(model_type, prediction, RF:GBM) %>%
group_by(model_type) %>%
summarize(rmse = rmse(pred = prediction, obs = test$price))
evalResults %>%
gather(model_type, prediction, RF:GBM) %>%
group_by(model_type) %>%
summarize(rmse = rmse(sim = prediction, obs = test$price))
evalResults %>%
gather(model_type, prediction, RF:GBM) %>%
group_by(model_type) %>%
summarize(rmse = rmse(sim = prediction, obs = validation$price))
dim(validation)
dim(evalResults)
?rmse
evalResults %>%
gather(model_type, prediction, RF:GBM) %>%
group_by(model_type) %>%
summarize(rmse = hydroGOF::rmse(sim = prediction, obs = validation$price))
evalResults %>%
gather(model_type, prediction, RF:GBM)
evalResults
rmse(sim = evalResults$GBM, obs = validation$price)
tibble(#LM = predict(lm.fit, newdata = validation),
#ELASTIC = predict.train(elastic.fit, newdata = validation),
RF = predict.train(rf.fit, newdata = validation) %>%
rmse(pred = ., validation$price),
GBM = predict.train(gbm.fit, newdata = validation) %>%
rmse(pred = ., validation$price))
tibble(#LM = predict(lm.fit, newdata = validation),
#ELASTIC = predict.train(elastic.fit, newdata = validation),
RF = predict.train(rf.fit, newdata = validation) %>%
rmse(sim = ., obs = validation$price),
GBM = predict.train(gbm.fit, newdata = validation) %>%
rmse(sim = ., obs = validation$price))
evalResults <- tibble(# LM = predict(lm.fit, newdata = validation) %>%
# rmse(sim = ., obs = validation$price),
# ELASTIC = predict.train(elastic.fit, newdata = validation) %>%
# rmse(sim = ., obs = validation$price),
RF = predict.train(rf.fit, newdata = validation) %>%
rmse(sim = ., obs = validation$price),
GBM = predict.train(gbm.fit, newdata = validation) %>%
rmse(sim = ., obs = validation$price))
evalResults %>%
gather(model_type, rmse, RF:GBM)
evalResults %>%
gather(model_type, rmse, RF:GBM) %>%
ggplot(mapping = aes(x = model_type, y = rmse)) +
geom_bar(stat = "identity")
evalResults %>%
gather(model_type, rmse, RF:GBM) %>%
ggplot(mapping = aes(x = model_type, y = rmse)) +
geom_bar(stat = "identity") +
scale_y_continuous(labels = scales::dollar)
lm.fit
summary(lm.fit())
summary(lm.fit
)
View(validation)
predict(lm.fit, validation)
train <- predict(preProcessObject, train)
validation <- predict(preProcessObject, validation)
lm.fit <- lm(price~., data = train)
predict(lm.fit, validation)
predict(lm.fit, validation) %>% rmse(., obs = validation$price)
predict(lm.fit, validation) %>5 View()
predict(lm.fit, validation) %>% View()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Initialize libraries
library(stringr)
library(hydroGOF)
library(lubridate)
library(knitr)
library(caret)
library(tidyverse)
# Define input file
file_in <- "../data/train.csv"
# Read in data
df <- read_csv(file_in) %>%
dmap_at("date", ~ymd(.x))
df <- df %>%
# Recode "waterfront" to factor
dmap_at("waterfront", as.factor) %>%
mutate(
years_since_renovation = ifelse(yr_renovated == 0, 0, yr_renovated - yr_built),
sale_year = year(date),
sale_month = month(date),
yard_size = sqft_lot - sqft_living,
house_age = sale_year - yr_built,
sale_season = ifelse(sale_month <= 4, "Winter",
ifelse(sale_month <= 5, "Spring",
ifelse(sale_month <= 8, "Summer",
ifelse(sale_month <= 12, "Fall")))),
size_vs_neighbors = sqft_living/sqft_living15,
price_per_sqft = price/sqft_living
) %>%
# Remove extraneoys variables
select(-c(id, date, yr_built, yr_renovated, zipcode, lat, long, sale_year, sale_month,
price_per_sqft, size_vs_neighbors))
# Split data into train and validation sets
percent_in_train <- 0.7
train_indicies <- sample(nrow(df), size = percent_in_train*nrow(df))
train <- df[train_indicies, ]
validation <- df[-train_indicies, ]
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale")
# Apply same pre-processing steps to validation set
preProcessObject <- preProcess(train, method = preProcessSteps)
train <- predict(preProcessObject, train)
validation <- predict(preProcessObject, validation)
View(train)
lm.fit <- lm(price~., data = train)
summary(lm.fit)
predict(lm.fit, validation)
library(tidyverse)
file_in <- "../data/house_data.csv"
# Read in data
df <- read_csv(file_in)
# Split data into 70% train, 30% test
percent_in_train <- 0.8
set.seed(1234)
train_indicies <- sample(nrow(df), size = percent_in_train*nrow(df))
train <- df[train_indicies, ]
test <- df[-train_indicies, ]
# Write files to CSV
write_csv(train, "../data/train.csv")
write_csv(test, "../data/test.csv")
