---
title: "MS&E 226 Mini-Project Part 2"
author: "Samuel Hansen & Sarah Rosston"
date: "11/13/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Initialize libraries
library(stringr)
library(hydroGOF)
library(lubridate)
library(knitr)
library(caret)
library(tidyverse)
# Define input file
file_in <- "../data/train.csv"
```

```{r}
# Read in data
df <- read_csv(file_in) %>%
  dmap_at("date", ~ymd(.x))
```

#Summary 

In part 2 of our mini-project, we built regression and classification models of 
`home price`. Our report describes the steps we took for data cleaning, 
pre-processing, feature selection, model fitting, and evaluation. 

#Data Cleaning

This section describes the steps we took to engineer features, include 
external variables, split, and preprocess our data prior to model building.

##Feature Engineering 
Prior to model building, we engineered the following features from 
raw values:

1) Years since renovation: `renovation_year` - `year_built`
2) House age at time of sale: `sale_year` - `year_built`
3) Season of sale: Fall (9 <= `sale_month` <= 12), Winter (1 <= `sale_month` <= 4), etc.
4) Price over median: Boolean stating whether `price > median(price)`

```{r}
df <- df %>%
  # Recode "waterfront" to factor
  dmap_at("waterfront", as.factor) %>%
  mutate(
    years_since_renovation = ifelse(yr_renovated == 0, 0, yr_renovated - yr_built),
    sale_year = year(date), 
    sale_month = month(date), 
    house_age = sale_year - yr_built,
    sale_season = ifelse(sale_month <= 4, "Winter", 
                    ifelse(sale_month <= 5, "Spring",
                           ifelse(sale_month <= 8, "Summer", 
                                  ifelse(sale_month <= 12, "Fall")))),
    # Defines binary response variable 
    price_over_median = ifelse(price > median(price), 1, 0)
  ) %>%
  # Remove extraneoys variables 
  select(-c(id, date, yr_built, yr_renovated, zipcode, lat, long, sale_year, sale_month))
```

##Added Features

SARAH:
- Median income
- School District rating (is easy)
- other useful things 

##Data Splitting 

We split our data to include 80% training and 20% validation sets. 
```{r}
# Split data into train and validation sets 
percent_in_train <- 0.7
train_indicies <- sample(nrow(df), size = percent_in_train*nrow(df))
train <- df[train_indicies, ]
validation <- df[-train_indicies, ]
```

##Data  Preprocessing

We center and scale the predictors in order to apply regularization 
techniques during the modeling phase. 
```{r}
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale")
# Apply same pre-processing steps to validation set
preProcessObject <- preProcess(train, method = preProcessSteps)
train <- predict(preProcessObject, train)
validation <- predict(preProcessObject, validation)
```

#Regression

We first aim to build a predictive model of `home price`. To do so,
we perform recusrive feature elimination to select our feature set,
fit 5 different predictive models using 10-fold cross-validation, 
then evaluate their performance on a held-out validation set to estimate the 
generalization error. 

##Feature Selection

We perform feature selection using recursive feature elimination
with 10-fold cross-validation. This method uses the 
`rfFuncs` parameter, which uses random forests to remove 
variables with low variable importance.
```{r}
set.seed(1234)
rfe.cntrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 10)

train.cntrl <- trainControl(selectionFunction = "oneSE")

# Commented out to speed up runtime 
# rfe.results <- rfe(price~., train,
#                rfeControl = rfe.cntrl,
#                preProc = preProcessSteps,
#                metric = "RMSE",
#               trControl = train.cntrl)
rfe.results <- read_rds("../models/rfe.results.rds")
```

The following table shows that recursive feature selection 
chooses all `r rfe.results[["results"]]$Variables[which.min(rfe.results[["results"]]$RMSE)]`
variables to include in subsequent model building.
```{r}
print(rfe.results)
```

The procedure selects `r rfe.results[["results"]]$Variables[which.max(rfe.results[["results"]]$RMSE)]` variables because RMSE is minimized (see plot below):
```{r}
ggplot(rfe.results) +
  labs(title = "Recursive Feature Elimination\nNumber of Variables vs. RMSE")
```

The variable importance of the predictors is shown below:
```{r}
data_frame(predictor = rownames(varImp(rfe.results)), 
           var_imp = varImp(rfe.results)$Overall) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", y = "Variable Importance", 
       title = "Recursive Feature Elimination Variable Importance")
```

We observe that house_age is, by far, the most important predictor of price
selected via cross-validated recursive feature elimination. 

##Model Fitting

We define the cross-validation controls as follows: 
```{r, echo = TRUE}
cvCtrl <- trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 3,
                       selectionFunction = "oneSE")
```

###Ordinary Least Squares Regression 

SARAH: 
- Include 2 linear models 
- consider interaction terms 
- look at residuals 
- remove outliers and update training data for future models 
```{r}
lm.fit1 <- lm(price~., data = train)
# lm.fit2 SOMETHING ELSE  
```

###Elastic Net Regularized Regression Model
```{r}
# # Define grid of tuning parameters
# # tuneGrid <- expand.grid(.alpha = seq(0, 1, 0.1),
# #                          .lambda = seq(0, 0.05, by = 0.005))
# # Fit penalized logistic regression model (elastic net)
# set.seed(1234)
# elastic.fit <- train(price ~ .,
#                    data = train,
#                    preProc = preProcessSteps,
#                    method = "glmnet",
#                    # tuneGrid = tuneGrid,
#                    trControl = cvCtrl)
elastic.fit <- read_rds("../models/elastic.fit.rds")
```

###Random Forest Model
```{r}
# Define tuning paramter grid
# rfGrid <- expand.grid(.mtry = c(4,5,6,7))
# Fit random forest model
# set.seed(1234)
# rf.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "rf",
#                 tuneGrid = rfGrid,
#                 trControl = cvCtrl)
rf.fit <- read_rds("../models/rf.fit.rds")
```

###Gradient Boosting Machine Model
```{r}
# # Define grid of tuning parameters
# gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
#                         n.trees = (1:20)*100,
#                         shrinkage = seq(.0005, .05, .005),
#                         n.minobsinnode = 10)
# # Fit GBM model
# set.seed(825)
# gbm.fit <- train(price ~ .,
#                 data = train,
#                 preProc = preProcessSteps,
#                 method = "gbm",
#                 tuneGrid = gbmGrid,
#                 trControl = cvCtrl)
gbm.fit <- read_rds("../models/gbm.fit.rds")
```

##Regression Evaluation 
```{r}
evalResults <- tibble(# LM = predict(lm.fit, newdata = validation) %>% 
                      # rmse(sim = ., obs = validation$price),
                      # ELASTIC = predict.train(elastic.fit, newdata = validation) %>% 
                      # rmse(sim = ., obs = validation$price),
                      RF = predict.train(rf.fit, newdata = validation) %>% 
                        rmse(sim = ., obs = validation$price),
                      GBM = predict.train(gbm.fit, newdata = validation) %>% 
                        rmse(sim = ., obs = validation$price))

evalResults %>%
  gather(model_type, rmse, RF:GBM) %>%
  ggplot(mapping = aes(x = model_type, y = rmse)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::dollar)
```

#Classification

SARAH: 
- Add 2 logistic regression models 
```{r}
# logit.fit1
# logit.fit2
```

SAM: 

#Model Fitting

We define the cross-validation controls as follows: 
```{r, echo = TRUE}
cvCtrl <- trainControl(method = "repeatedcv", 
                       number = 10,
                       repeats = 3,
                       summaryFunction = twoClassSummary, 
                       selectionFunction = "oneSE",
                       classProbs = TRUE)
```

##Random Forest Model
```{r}
# Define tuning paramter grid
rfGrid <- expand.grid(.mtry = c(4,5,6,7))
# Fit random forest model
set.seed(1234)
rf.fit <- train(price ~ .,
                data = train,
                preProc = preProcessSteps,
                method = "rf",
                tuneGrid = rfGrid,
                trControl = cvCtrl,
                na.action = na.roughfix,
                metric = "ROC")
```

##Gradient Boosting Machine 
```{r}
# Define tuning paramter grid
gbmGrid <- expand.grid(interaction.depth = c(1, 2, 3),
                        n.trees = (1:20)*100,
                        shrinkage = seq(.0005, .05, .005),
                        n.minobsinnode = 10)
# Fit GBM model
set.seed(1234)
gbm.fit <- train(price ~ .,
                data = train,
                preProc = preProcessSteps,
                method = "gbm",
                tuneGrid = gbmGrid,
                trControl = cvCtrl,
                na.action = na.roughfix,
                metric = "ROC")
```

##Support Vector Machine with Radial Kernel
```{r}
# Define tuning paramter grid
svmGrid <- expand.grid(.sigma = 0.003408979,
                       .C = c(0.005, 0.05, 0.15, 0.25, 0.35, 0.45, 0.5))
# Fit SVM model
set.seed(1234)
svm.fit <- train(price ~ .,
                data = train,
                preProc = preProcessSteps,
                method = "svmRadial",
                tuneGrid = svmGrid,
                trControl = cvCtrl,
                na.action = na.roughfix,
                metric = "ROC")
```


##Classification Evaluation 

```{r}
# Evaluate all models on held-out validaion set 
evalResults <- data.frame(dead6m = validation$price_over_med)
evalResults$LOGIT <- predict(logit.fit, validation, type = "prob")
evalResults$GBM <- predict(gbm.fit, validation, type = "prob")
evalResults$SVM <- predict(svm.fit, validation, type = "prob")
evalResults$RF <- predict(rf.fit, validation, type = "prob")
evalResults <- evalResults %>%
  gather(model_type, predicted_prob, LOGIT:RF) 
```


##Calibration Plots

```{r}
# Make calibration plots, facetted by model type
model_labels <- c("LOGIT" = "Logistic Regression",
                  "SVM" = "Support Vector Machine", 
                  "GBM" = "Gradient Boosting Machine",
                  "RF" = "Random Forest")
evalResults %>%
  mutate(prob_bin = cut(predicted_prob, breaks = seq(-0.05, 1.0, by = 0.05))) %>% 
  group_by(prob_bin, model_type) %>%
  summarise(prob_high_price = mean(price, na.rm = TRUE),
            n = n()) %>%
  filter(!is.na(prob_bin)) %>% 
  bind_cols(., pred_prob_midpoints) %>%
  ungroup() %>%
  ggplot(mapping = aes(x = midpoint, y = prob_high_price, label = n)) +
  geom_line() +
  geom_point(mapping = aes(size = n)) +
  geom_text_repel(mapping = aes(color = "red")) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1, 
           color = "black", linetype = 2) +
  scale_x_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_y_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_colour_discrete(guide = FALSE) +
  scale_size(name = "Number of\nPredictions",
             labels = scales::comma) +
  labs(x = "Predicted Probability Price > Median (Bin Midpoint)",
       y = "Actual Probability Price > Median",
       title = "Calibration Plot: Predicted vs. Actual Probability Home Price > Median") +
  facet_wrap(~model_type, labeller = labeller(model_type = model_labels))
```

